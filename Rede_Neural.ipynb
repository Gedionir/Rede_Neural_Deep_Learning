{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+JaStbEDvIZnrC2W5D2w/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gedionir/Rede_Neural_Deep_Learning/blob/main/Rede_Neural.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**REDE DE DEEP LEARNING PARA O RECONHECIMENTO DE DIGITO.**"
      ],
      "metadata": {
        "id": "2wq4LWCMqHof"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "PU-7WnV45lCd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definindo a conversão das imagens do datasets para tensor."
      ],
      "metadata": {
        "id": "XC_erm5Ipmyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.ToTensor() #definindo a conversão de imagem para tensor\n",
        "\n",
        "trainset = datasets.MNIST('./MNIST_data/', download=True, train=True, transform=transform) # Carrega a parte de treino do dataset.\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) # Cria um buffer para pegar dados por partes.\n",
        "\n",
        "valset = datasets.MNIST('./MNIST_data/', download=True, train=False, transform=transform) # Carrega a parte de validação do dataset.\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True) # Cria um buffer para pegar dados por partes."
      ],
      "metadata": {
        "id": "SiKIxGD8KMIr"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(trainloader)\n",
        "imagens, etiquetas = next(dataiter)\n",
        "plt.imshow(imagens[29].numpy().squeeze(), cmap='gray_r');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "YkINIGLHN9D8",
        "outputId": "8e51a2b3-dd01-46a7-c5c1-53db12e9249b"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(imagens[0].shape) #para verificar as dimensões do tensor de cada imagem.\n",
        "print(etiquetas[0].shape) #para verificar as dimensões do tensor de cada etiqueta."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBa5DrifScqk",
        "outputId": "fc2a7508-6c24-43ec-f98e-d46a4f5c4645"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "torch.Size([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Modelo(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Modelo, self).__init__()\n",
        "    self.linear1 == nn.Linear(28*28, 128) # cada camada de entrada, 784 neurônios que se ligam a 128 neurônios.\n",
        "    self.linear2 == nn.Linear(128, 64) # camada interna 1, 128 neurônios que se ligam a 64 neurônios.\n",
        "    self.linear3 == nn.Linear(64, 10) # camada interna 2, 64 neurônios que se ligam a 10 .\n",
        "    # Para cada camada de saída não é necessário definir nada , pois só precisamos pegar o output da camada interna 2.\n",
        "\n",
        "  def forward(self, X):\n",
        "    X = F. relu(self.linear1(X)) # função de ativação da camada de entrada para cada camada interna 1.\n",
        "    X = F. relu(self.linear2(X)) # função de ativação da camada de entrada 1 para cada camada interna 2.\n",
        "    X = self.linear3(x) #função de ativação da camda de entrada 2 para a camada de saída, neste caso f(x) = x\n",
        "    return F.log_softmax(X, din=1) # dados utilizados para calcular a perda."
      ],
      "metadata": {
        "id": "Y9fkxTTOBDgH"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XeV8x6bZhzZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelo para treinar."
      ],
      "metadata": {
        "id": "LSH-0dylpWjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def treino(modelo, trainloader, device):\n",
        "\n",
        "  otimizador = optim.SGD(modelo.parameters(), lr=0.01, momentum=0.5) # define a política de atualização dos pesos e da bias.\n",
        "  inicio : float\n",
        "  inicio = time() # timer para sabermos quanto tempo levou o treino.\n",
        "\n",
        "  criterio = nn.NLLLoss() # definindo critérios para calcular as perdas.\n",
        "  EPOCHS = 10 # numéros de epochs que o algoritmos rodará.\n",
        "  modelo.train() # ativando o modo de treinamento do modelo.\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    perda_acumulada = 0 # inicialização da perda acumulada da epoch em questão.\n",
        "\n",
        "    for imagens, etiquetas in trainloader:\n",
        "      imagens = imagens.view(imagens.shape[0], -1) # convertendo as imagens para vetores de 28*28 casas para ficarem compativeis com a\n",
        "      otimizador.zero_grad() # zerando os gradientes por conta do ciclo anterior.\n",
        "\n",
        "      output = modelo(imagens.to(device)) # colocando dados no modelo.\n",
        "      perda_instantanea = criterio(output, etiquetas.to(device)) #  calculando a perda da epoch em questão.\n",
        "\n",
        "      perda_instantanea.backward()  # back propagation a partir da perda.\n",
        "\n",
        "      otimizador.step() # atualizandos os pesos e as bias.\n",
        "\n",
        "      perda_acumulada += perda_instantanea.item() # atualização da perda acumulada.\n",
        "\n",
        "\n",
        "    else:\n",
        "        print(\"Epoch {} - Perda resultante: {}\".format(epoch+1, perda_acumulada/len(trainloader)))\n",
        "        print(\"\\nTempo de treino (em minutos) =\",(time()-inicio)/60)\n"
      ],
      "metadata": {
        "id": "BSb6_74pE3Nx"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rodar a validação. Algoritmo vai verificar a base de dados do treino com o que esta acontecendo com o treinamento."
      ],
      "metadata": {
        "id": "HUBXog9tkNYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validacao(modelo, valloader, device):\n",
        "  conta_corretas, conta_todas = 0, 0\n",
        "  for imagens, estiquetas in valloader:\n",
        "    for i in range(len(etiquetas)):\n",
        "      img = imagens[i].view(1, 784) # Desativar o autograd para acelerar a validação. Grafos computacionais dinâmicos tem um custo alto de processamento.\n",
        "      with torch.no_grad():\n",
        "        logps = modelo(img.to(device)) # output do modelo em escala logarítmica.\n",
        "\n",
        "      ps = torch.exp(logps) # Converte output para escala normal(lembrando que é um tensor).\n",
        "      probab = list(ps.cpu().numpy()[0])\n",
        "      etiqueta_pred = probab.index(max(probab)) # converte um tensor em um número, no caso, o número que o modelo previu como correto.\n",
        "      etiqueta_certa = etiquetas.numpy()[i]\n",
        "      if(etiqueta_certa == etiqueta_pred): # compara a previsão com o valor correto.\n",
        "        conta_corretas += 1\n",
        "      conta_todas += 1\n",
        "\n",
        "  print(\"Total de imagens testadas =\", conta_todas)\n",
        "  print(\"\\nPrecisão do modelo = {}%\".format(conta_corretas*100/conta_todas))\n"
      ],
      "metadata": {
        "id": "5cNaxmg-lrUQ"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#modelo = Modelo()\n",
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#modelo.to(device)\n"
      ],
      "metadata": {
        "id": "smc9Ej6qiylf"
      },
      "execution_count": 114,
      "outputs": []
    }
  ]
}